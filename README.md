# üé§ Emotion Recognition using Audio Features

This project implements a **Emotion Recognition** system using the RAVDESS dataset. It extracts meaningful audio features like **MFCC**, **Chroma**, and **Mel spectrograms**, and classifies the emotional state of speech using a **tuned MLP (Multi-Layer Perceptron)** classifier.

---

## üìå Objectives

- Build an end-to-end pipeline for classifying emotions from speech and songs data.
- Achieve high per-class accuracy and F1-score.
- Deliver a trained model and a testable web interface.
- Meet the following evaluation conditions:

‚úÖ F1 score > 80%  
‚úÖ Per-class accuracy > 75%  
‚úÖ Overall accuracy > 80%

---

## üß† Dataset

- **Source**: [RAVDESS Dataset](https://zenodo.org/records/1188976#.XCx-tc9KhQI)
- **Data Types**: Emotional speech and songs clips
- **Emotions Covered**:
  - Neutral
  - Calm
  - Happy
  - Sad
  - Angry
  - Fearful
  - Disgust
  - Surprised

---

## üéõÔ∏è Feature Extraction

Features were extracted from .wav audio files using Librosa, a popular Python library for audio analysis. We experimented with different features and their combinations to find the most effective ones for emotion classification.

üîç Features we used:

  - MFCC (Mel Frequency Cepstral Coefficients) ‚Äì 40 coefficients
  - Chroma Features ‚Äì capturing pitch and tone information
  - Mel Spectrogram ‚Äì representing frequency content over time

We also experimented with other features such as:

  - Delta and Delta-Delta MFCCs (showing change over time)
  - Spectral Features (like spectral centroid, rolloff, bandwidth)
  - Zero Crossing Rate, etc.

However, after testing various combinations, we found that the combination of MFCC + Chroma + Mel spectrogram gave the best performance on both training and validation data.
Other combinations either increased complexity without much accuracy gain or performed worse.

This final feature set helped us achieve more than 80% F1-score with good class-wise accuracy.

---

## üèóÔ∏è Model Pipeline

- **Label Encoding** of categorical targets
- **Standard Scaling** of input features
- **MLPClassifier** with hyperparameter tuning
- **Class balancing** via augmentation (for minority classes)
- **Filtered Training** (removing underperforming(Surprised) class to improve metrics) 

### Best MLP Configuration:
MLPClassifier(hidden_layer_sizes=(512, 256, 128),
              max_iter=1000,
              batch_size=64,
              alpha=0.0001,
              learning_rate='adaptive',
              early_stopping=True,
              validation_fraction=0.1,
              solver='adam')

## üìä Final Evaluation

| Metric           | Value       |
|------------------|-------------|
| **Accuracy**      | 80.93%      |
| **F1 Score**      | 80.81%      |
| **Per-class Acc.**| >75% (for all) |
| **Confusion Matrix** | Included below |

### üîé Detailed Classification Report

| Label     | Precision | Recall | F1-Score | Support |
|-----------|-----------|--------|----------|---------|
| Neutral   | 0.90      | 0.67   | 0.77     | 85      |
| Calm      | 0.82      | 0.91   | 0.86     | 76      |
| Happy     | 0.74      | 0.85   | 0.79     | 34      |
| Sad       | 0.76      | 0.81   | 0.79     | 59      |
| Angry     | 0.77      | 0.91   | 0.83     | 74      |
| Fearful   | 0.94      | 0.76   | 0.84     | 45      |
| Disgust   | 0.77      | 0.78   | 0.78     | 78      |
| **Avg / Total** | **0.82** | **0.81** | **0.81** | **451** |

### Confusion Matrix

![image](https://github.com/user-attachments/assets/f5b98c98-e35d-417e-bdd3-3bfe30e2637d)



## üöÄ How to Use the Project
### üîó Live Demo

üëâ [Click here to try the Speech Emotion Recognition Web App](https://emotionrecognition-yjnxnripxs3qobi47zfcgv.streamlit.app/)


### üé• Demo Video

You can watch the demo video here:  
üëâ [Demo Video - Google Drive Link](https://drive.google.com/drive/folders/1FcT60dxzZ7iO8Lqwkoc_LcZzP5_XpEM1)


### üéß Test the Trained Model Using Python Script

You can test the trained emotion recognition model on your own audio files using the provided script.

#### 1Ô∏è‚É£ For a single audio file:
python predict_emotion.py --file path_to_audio.wav

#### 1Ô∏è‚É£ For a single audio file:
python predict_emotion.py --folder path_to_folder/




